# Jobs database credentials
POSTGRES_USER=jobs_user
POSTGRES_PASSWORD=jobs_password
POSTGRES_DB=jobs_db

# Jobs database credentials
DB_NAME=jobs_db
DB_USER=jobs_user
DB_PASSWORD=jobs_password
DB_HOST=jobs_db
DB_PORT=5433

# Path to pocessed data inside container
# needs to be consitent with CONTAINER_OUTPUT var in airflow .env
OEO_JOB_DATA=/data_out

# Apache Airflow URL (leave unchanged if using the dockerized version)
OEO_AIRFLOW_HOST=http://airflow-webserver:8080

# Delay after which to delete sync-jobs output
# It must be minimum above the timeout of gunicorn and nginx
OEO_SYNC_DEL_DELAY=300 # seconds


# --------------------------------------- #
# Export equivalent env vars (needed for to run nameko serices locally without docker containers)
# Note: "export" lines are ignored by docker-compose

# Jobs database credentials
export POSTGRES_USER=jobs_user
export POSTGRES_PASSWORD=jobs_password
export POSTGRES_DB=jobs_db

# Jobs database credentials
export DB_NAME=jobs_db
export DB_USER=jobs_user
export DB_PASSWORD=jobs_password
export DB_HOST=localhost # leave this as localhost
export DB_PORT=5433

# Path to pocessed data inside container
# needs to be consitent with CONTAINER_OUTPUT var in airflow .env
export JOB_DATA=/data_out

# Apache Airflow URL (leave unchanged if using the dockerized version)
export AIRFLOW_HOST=http://airflow_webserver:8080

# Delay after which to delete sync-jobs output
# It must be minimum above the timeout of gunicorn and nginx
export SYNC_DEL_DELAY=300 # seconds
